{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TensorFlow Assignment: Multilayer Perceptron (MLP) and Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Duke Community Standard](http://integrity.duke.edu/standard.html): By typing your name below, you are certifying that you have adhered to the Duke Community Standard in completing this assignment.**\n",
    "\n",
<<<<<<< HEAD
    "Name: [ Leida Ren ]"
=======
    "Name: [YOUR_NAME_HERE]"
>>>>>>> 8110aca4ac970539383668359a4b97e9f98b3437
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you've run through a simple logistic regression model on MNIST, let's see if we can do better (Hint: we can). For this assignment, you'll build a multilayer perceptron (MLP) and a convolutional neural network (CNN), two popular types of neural networks, and compare their performance. Some potentially useful code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "# Import data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Helper functions for creating weight variables\n",
    "def weight_variable(shape):\n",
    "    \"\"\"weight_variable generates a weight variable of a given shape.\"\"\"\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "def bias_variable(shape):\n",
    "    \"\"\"bias_variable generates a bias variable of a given shape.\"\"\"\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "# Tensorflow Functions that might also be of interest:\n",
    "# tf.nn.sigmoid()\n",
    "# tf.nn.relu()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multilayer Perceptron\n",
    "\n",
    "Build a multilayer perceptron for MNIST digit classfication. Feel free to play around with the model architecture and see how the training time/performance changes, but to begin, try the following:\n",
    "\n",
    "Image -> fully connected (500 hidden units) -> nonlinearity (Sigmoid/ReLU) -> fully connected (10 hidden units) -> softmax\n",
    "\n",
    "Skeleton framework for you to fill in (Code you need to provide is marked by `###`):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, validation accuracy 0.098\n",
      "step 250, validation accuracy 0.56\n",
      "step 500, validation accuracy 0.711\n",
      "step 750, validation accuracy 0.762\n",
      "step 1000, validation accuracy 0.814\n",
      "step 1250, validation accuracy 0.804\n",
      "step 1500, validation accuracy 0.837\n",
      "step 1750, validation accuracy 0.821\n",
      "step 2000, validation accuracy 0.833\n",
      "step 2250, validation accuracy 0.818\n",
      "step 2500, validation accuracy 0.838\n",
      "step 2750, validation accuracy 0.809\n",
      "step 3000, validation accuracy 0.815\n",
      "step 3250, validation accuracy 0.821\n",
      "step 3500, validation accuracy 0.824\n",
      "step 3750, validation accuracy 0.818\n",
      "step 4000, validation accuracy 0.844\n",
      "step 4250, validation accuracy 0.829\n",
      "step 4500, validation accuracy 0.808\n",
      "step 4750, validation accuracy 0.832\n",
      "step 5000, validation accuracy 0.846\n",
      "step 5250, validation accuracy 0.85\n",
      "step 5500, validation accuracy 0.825\n",
      "step 5750, validation accuracy 0.845\n",
      "step 6000, validation accuracy 0.841\n",
      "step 6250, validation accuracy 0.814\n",
      "step 6500, validation accuracy 0.852\n",
      "step 6750, validation accuracy 0.841\n",
      "step 7000, validation accuracy 0.845\n",
      "step 7250, validation accuracy 0.856\n",
      "step 7500, validation accuracy 0.828\n",
      "step 7750, validation accuracy 0.809\n",
      "step 8000, validation accuracy 0.846\n",
      "step 8250, validation accuracy 0.831\n",
      "step 8500, validation accuracy 0.847\n",
      "step 8750, validation accuracy 0.85\n",
      "step 9000, validation accuracy 0.844\n",
      "step 9250, validation accuracy 0.826\n",
      "step 9500, validation accuracy 0.862\n",
      "step 9750, validation accuracy 0.843\n",
      "step 10000, validation accuracy 0.841\n",
      "step 10250, validation accuracy 0.844\n",
      "step 10500, validation accuracy 0.826\n",
      "step 10750, validation accuracy 0.856\n",
      "step 11000, validation accuracy 0.858\n",
      "step 11250, validation accuracy 0.834\n",
      "step 11500, validation accuracy 0.838\n",
      "step 11750, validation accuracy 0.845\n",
      "step 12000, validation accuracy 0.873\n",
      "step 12250, validation accuracy 0.845\n",
      "step 12500, validation accuracy 0.85\n",
      "step 12750, validation accuracy 0.845\n",
      "step 13000, validation accuracy 0.825\n",
      "step 13250, validation accuracy 0.866\n",
      "step 13500, validation accuracy 0.852\n",
      "step 13750, validation accuracy 0.843\n",
      "step 14000, validation accuracy 0.845\n",
      "step 14250, validation accuracy 0.845\n",
      "step 14500, validation accuracy 0.86\n",
      "step 14750, validation accuracy 0.85\n",
      "step 15000, validation accuracy 0.843\n",
      "step 15250, validation accuracy 0.864\n",
      "step 15500, validation accuracy 0.841\n",
      "step 15750, validation accuracy 0.857\n",
      "step 16000, validation accuracy 0.881\n",
      "step 16250, validation accuracy 0.849\n",
      "step 16500, validation accuracy 0.844\n",
      "step 16750, validation accuracy 0.846\n",
      "step 17000, validation accuracy 0.88\n",
      "step 17250, validation accuracy 0.851\n",
      "step 17500, validation accuracy 0.842\n",
      "step 17750, validation accuracy 0.863\n",
      "step 18000, validation accuracy 0.839\n",
      "step 18250, validation accuracy 0.837\n",
      "step 18500, validation accuracy 0.857\n",
      "step 18750, validation accuracy 0.871\n",
      "step 19000, validation accuracy 0.857\n",
      "step 19250, validation accuracy 0.861\n",
      "step 19500, validation accuracy 0.855\n",
      "step 19750, validation accuracy 0.827\n",
      "step 20000, validation accuracy 0.866\n",
      "step 20250, validation accuracy 0.86\n",
      "step 20500, validation accuracy 0.87\n",
      "step 20750, validation accuracy 0.863\n",
      "step 21000, validation accuracy 0.85\n",
      "step 21250, validation accuracy 0.85\n",
      "step 21500, validation accuracy 0.866\n",
      "step 21750, validation accuracy 0.873\n",
      "step 22000, validation accuracy 0.851\n",
      "step 22250, validation accuracy 0.862\n",
      "step 22500, validation accuracy 0.867\n",
      "step 22750, validation accuracy 0.852\n",
      "step 23000, validation accuracy 0.853\n",
      "step 23250, validation accuracy 0.87\n",
      "step 23500, validation accuracy 0.867\n",
      "step 23750, validation accuracy 0.844\n",
      "step 24000, validation accuracy 0.864\n",
      "step 24250, validation accuracy 0.877\n",
      "step 24500, validation accuracy 0.862\n",
      "step 24750, validation accuracy 0.86\n",
      "step 25000, validation accuracy 0.856\n",
      "step 25250, validation accuracy 0.851\n",
      "step 25500, validation accuracy 0.87\n",
      "step 25750, validation accuracy 0.885\n",
      "step 26000, validation accuracy 0.856\n",
      "step 26250, validation accuracy 0.866\n",
      "step 26500, validation accuracy 0.861\n",
      "step 26750, validation accuracy 0.843\n",
      "step 27000, validation accuracy 0.869\n",
      "step 27250, validation accuracy 0.841\n",
      "step 27500, validation accuracy 0.868\n",
      "step 27750, validation accuracy 0.856\n",
      "step 28000, validation accuracy 0.872\n",
      "step 28250, validation accuracy 0.855\n",
      "step 28500, validation accuracy 0.859\n",
      "step 28750, validation accuracy 0.873\n",
      "step 29000, validation accuracy 0.879\n",
      "step 29250, validation accuracy 0.844\n",
      "step 29500, validation accuracy 0.871\n",
      "step 29750, validation accuracy 0.869\n",
      "step 30000, validation accuracy 0.876\n",
      "step 30250, validation accuracy 0.854\n",
      "step 30500, validation accuracy 0.867\n",
      "step 30750, validation accuracy 0.857\n",
      "step 31000, validation accuracy 0.86\n",
      "step 31250, validation accuracy 0.865\n",
      "step 31500, validation accuracy 0.862\n",
      "step 31750, validation accuracy 0.884\n",
      "step 32000, validation accuracy 0.853\n",
      "step 32250, validation accuracy 0.851\n",
      "step 32500, validation accuracy 0.882\n",
      "step 32750, validation accuracy 0.867\n",
      "step 33000, validation accuracy 0.852\n",
      "step 33250, validation accuracy 0.854\n",
      "step 33500, validation accuracy 0.874\n",
      "step 33750, validation accuracy 0.885\n",
      "step 34000, validation accuracy 0.85\n",
      "step 34250, validation accuracy 0.877\n",
      "step 34500, validation accuracy 0.87\n",
      "step 34750, validation accuracy 0.856\n",
      "step 35000, validation accuracy 0.862\n",
      "step 35250, validation accuracy 0.863\n",
      "step 35500, validation accuracy 0.85\n",
      "step 35750, validation accuracy 0.85\n",
      "step 36000, validation accuracy 0.888\n",
      "step 36250, validation accuracy 0.851\n",
      "step 36500, validation accuracy 0.877\n",
      "step 36750, validation accuracy 0.861\n",
      "step 37000, validation accuracy 0.863\n",
      "step 37250, validation accuracy 0.869\n",
      "step 37500, validation accuracy 0.855\n",
      "step 37750, validation accuracy 0.877\n",
      "step 38000, validation accuracy 0.864\n",
      "step 38250, validation accuracy 0.86\n",
      "step 38500, validation accuracy 0.871\n",
      "step 38750, validation accuracy 0.87\n",
      "step 39000, validation accuracy 0.871\n",
      "step 39250, validation accuracy 0.867\n",
      "step 39500, validation accuracy 0.964\n",
      "step 39750, validation accuracy 0.964\n",
      "test accuracy 0.9616\n"
     ]
    }
   ],
   "source": [
    "# Model Inputs\n",
    "#x = mnist.train.images ### MNIST images enter graph here ###\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y_ =tf.placeholder(tf.float32, [None, 10])\n",
    "#mnist.train.labels ### MNIST labels enter graph here ###\n",
    "\n",
    "# Define the graph\n",
    "# refer to [02] jupter notebook\n",
    "weights_0 = tf.Variable(tf.truncated_normal([784, 500], dtype=tf.float32, stddev=1e-1), name='weights_0')\n",
    "biases_0 = tf.Variable(tf.constant(0.0, shape=[500], dtype=tf.float32), trainable=True, name='biases_0')\n",
    "FC_0 = tf.matmul(x, weights_0) + biases_0\n",
    "\n",
    "NL_0 = tf.nn.sigmoid(FC_0) # nonlinearity (Sigmoid/ReLU)\n",
    "\n",
    "weights_1 = tf.Variable(tf.truncated_normal([500,10], dtype=tf.float32, stddev=1e-1), name='weights_1')\n",
    "biases_1 = tf.Variable(tf.constant(0.0, shape=[10], dtype=tf.float32), trainable=True, name='biases_1')\n",
    "FC_1 = tf.matmul(NL_0, weights_1) + biases_1\n",
    "\n",
    "y_mlp = tf.nn.softmax(FC_1)# softmax\n",
    "### Create your MLP here##\n",
    "\n",
    "### Make sure to name your MLP output as y_mlp ###\n",
    "\n",
    "\n",
    "\n",
    "# Loss \n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_mlp))\n",
    "\n",
    "# Optimizer\n",
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)\n",
    "\n",
    "# Evaluation\n",
    "correct_prediction = tf.equal(tf.argmax(y_mlp, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initialize all variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training regimen\n",
    "    for i in range(40000):\n",
    "        # Validate every 250th batch\n",
    "        if i % 250 == 0:\n",
    "            validation_accuracy = 0\n",
    "            for v in range(10):\n",
    "                batch = mnist.validation.next_batch(100)\n",
    "                feed_dict = {x : batch[0], y_: batch[1]}\n",
    "                validation_accuracy += (1/10) * accuracy.eval(feed_dict=feed_dict)\n",
    "            print('step %d, validation accuracy %g' % (i, validation_accuracy))\n",
    "        \n",
    "        # Train    \n",
    "        batch = mnist.train.next_batch(50)\n",
    "        train_step.run(feed_dict={x: batch[0], y_: batch[1]})\n",
    "\n",
    "    print('test accuracy %g' % accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison\n",
    "\n",
    "How do the sigmoid and rectified linear unit (ReLU) compare?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### RELU\n",
    "- step 0, validation accuracy 0.072\n",
    "- step 250, validation accuracy 0.887\n",
    "- step 500, validation accuracy 0.922\n",
    "- step 750, validation accuracy 0.946\n",
    "- step 1000, validation accuracy 0.941\n",
    "- step 1250, validation accuracy 0.962\n",
    "- step 1500, validation accuracy 0.946\n",
    "- step 1750, validation accuracy 0.943\n",
    "- step 2000, validation accuracy 0.948\n",
    "- step 2250, validation accuracy 0.951\n",
    "- step 2500, validation accuracy 0.953\n",
    "- step 2750, validation accuracy 0.955\n",
    "- step 3000, validation accuracy 0.966\n",
    "- step 3250, validation accuracy 0.97\n",
    "- step 3500, validation accuracy 0.963\n",
    "- step 3750, validation accuracy 0.968\n",
    "- test accuracy 0.9674\n",
    "\n",
    "### sigmoid\n",
    "- step 0, validation accuracy 0.084\n",
    "- step 250, validation accuracy 0.697\n",
    "- step 500, validation accuracy 0.742\n",
    "- step 750, validation accuracy 0.779\n",
    "- step 1000, validation accuracy 0.729\n",
    "- step 1250, validation accuracy 0.737\n",
    "- step 1500, validation accuracy 0.737\n",
    "- step 1750, validation accuracy 0.749\n",
    "- step 2000, validation accuracy 0.781\n",
    "- step 2250, validation accuracy 0.75\n",
    "- step 2500, validation accuracy 0.725\n",
    "- step 2750, validation accuracy 0.774\n",
    "- step 3000, validation accuracy 0.766\n",
    "- step 3250, validation accuracy 0.745\n",
    "- step 3500, validation accuracy 0.754\n",
    "- step 3750, validation accuracy 0.761\n",
    "- test accuracy 0.7559\n",
    "\n",
    "### Conclusion\n",
    "1. Relu have a explicted higher accuracy than softmax from the second step to the final result\n",
    "2. I expanding the iter num of sigmoid to 40000, it shows a better performance but not as  **STABLE** as relu\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network\n",
    "\n",
    "Build a simple 2-layer CNN for MNIST digit classfication. Feel free to play around with the model architecture and see how the training time/performance changes, but to begin, try the following:\n",
    "\n",
    "Image -> CNN (32 5x5 filters) -> nonlinearity (ReLU) ->  (2x2 max pool) -> CNN (64 5x5 filters) -> nonlinearity (ReLU) -> (2x2 max pool) -> fully connected (1024 hidden units) -> nonlinearity (ReLU) -> fully connected (10 hidden units) -> softmax\n",
    "\n",
    "Some additional functions that you might find helpful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convolutional neural network functions\n",
    "def conv2d(x, W):\n",
    "    \"\"\"conv2d returns a 2d convolution layer with full stride.\"\"\"\n",
    "    return tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    \"\"\"max_pool_2x2 downsamples a feature map by 2X.\"\"\"\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "# Tensorflow Function that might also be of interest:\n",
    "# tf.reshape()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Skeleton framework for you to fill in (Code you need to provide is marked by `###`):\n",
    "\n",
    "*Hint: Convolutional Neural Networks are spatial models. You'll want to transform the flattened MNIST vectors into images for the CNN. Similarly, you might want to flatten it again sometime before you do a softmax. You also might want to look into the  [conv2d() documentation](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d) to see what shape kernel/filter it's expecting.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Model Inputs\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y_ =tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "xt = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "\n",
    "W1 = tf.Variable(tf.random_normal([5,5,1,32])) # 1 => 32\n",
    "conv1 = conv2d(xt, W1)\n",
    "relu1 = tf.nn.relu(conv1)\n",
    "pool1 = max_pool_2x2(relu1)\n",
    "\n",
    "W2 = tf.Variable(tf.random_normal([5,5,32,64])) # 32 => 64\n",
    "conv2 = conv2d(pool1, W2)\n",
    "relu2 = tf.nn.relu(conv2)\n",
    "pool2 = max_pool_2x2(relu2)\n",
    "\n",
    "crp = tf.reshape(pool2, shape=[-1, 7 * 7 * 64])\n",
    "# Define the graph\n",
    "# refer to [02] jupter notebook\n",
    "weights_0 = tf.Variable(tf.truncated_normal([7 * 7 * 64, 1024], dtype=tf.float32, stddev=1e-1), name='weights_0')\n",
    "biases_0 = tf.Variable(tf.constant(0.0, shape=[1024], dtype=tf.float32), trainable=True, name='biases_0')\n",
    "FC_0 = tf.matmul(crp, weights_0) + biases_0\n",
    "\n",
    "NL_0 = tf.nn.relu(FC_0) # nonlinearity (Sigmoid/ReLU)\n",
    "\n",
    "weights_1 = tf.Variable(tf.truncated_normal([1024,10], dtype=tf.float32, stddev=1e-1), name='weights_1')\n",
    "biases_1 = tf.Variable(tf.constant(0.0, shape=[10], dtype=tf.float32), trainable=True, name='biases_1')\n",
    "FC_1 = tf.matmul(NL_0, weights_1) + biases_1\n",
    "\n",
    "y_conv = tf.nn.softmax(FC_1)# softmax\n",
    "### Create your CNN here##\n",
    "### Make sure to name your CNN output as y_conv ###\n",
    "\n",
    "\n",
    "\n",
    "# Loss \n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "\n",
    "# Optimizer\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "# Evaluation\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initialize all variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training regimen\n",
    "    for i in range(10000):\n",
    "        # Validate every 250th batch\n",
    "        if i % 250 == 0:\n",
    "            validation_accuracy = 0\n",
    "            for v in range(10):\n",
    "                batch = mnist.validation.next_batch(50)\n",
    "                validation_accuracy += (1/10) * accuracy.eval(feed_dict={x: batch[0], y_: batch[1]})\n",
    "            print('step %d, validation accuracy %g' % (i, validation_accuracy))\n",
    "        \n",
    "        # Train    \n",
    "        batch = mnist.train.next_batch(50)\n",
    "        train_step.run(feed_dict={x: batch[0], y_: batch[1]})\n",
    "\n",
    "    print('test accuracy %g' % accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some differences from the logistic regression model to note:\n",
    "\n",
    "- The CNN model might take a while to train. Depending on your machine, you might expect this to take up to half an hour. If you see your validation performance start to plateau, you can kill the training.\n",
    "\n",
    "- The logistic regression model we used previously was pretty basic, and as such, we were able to get away with using the GradientDescentOptimizer, which performs implements the gradient descent algorithm. For more difficult optimization spaces (such as the ones deep networks pose), we might want to use more sophisticated algorithms. Prof David Carlson has a lecture on this later.\n",
    "    \n",
    "- Because of the larger size of our network, notice that our minibatch size has shrunk.\n",
    "    \n",
    "- We've added a validation step every 250 minibatches. This let's us see how our model is doing during the training process, rather than sit around twiddling our thumbs and hoping for the best when training finishes. This becomes especially significant as training regimens start approaching days and weeks in length. Normally, we validate on the entire validation set, but for the sake of time we'll just stick to 10 validation minibatches (500 images) for this homework assignment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Comparison\n",
    "\n",
    "How do the MLP and CNN compare in accuracy? Training time? Why would you use one vs the other? Is there a problem you see with MLPs when applied to other image datasets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "1. accuracy: CNN should much higher than MLP, however in my experience it shows bad, i think it is because of the **lack of bias for conv layers**.\n",
    "2. Training time; CNN is longer than MLP (especially running on my CPU machine)\n",
    "3. one vs the other: to modeling distance ignoring L1 difference(7 to 1 should same as 2 to 1)\n",
    "4. problem: Sorry for I have not try MLP on other datasets, but I guess it express too much detail without pooling layers.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, validation accuracy 0.09\n",
      "step 250, validation accuracy 0.676\n",
      "step 500, validation accuracy 0.808\n",
      "step 750, validation accuracy 0.872\n",
      "step 1000, validation accuracy 0.9\n",
      "step 1250, validation accuracy 0.97\n",
      "step 1500, validation accuracy 0.97\n",
      "step 1750, validation accuracy 0.976\n",
      "step 2000, validation accuracy 0.98\n",
      "step 2250, validation accuracy 0.982\n",
      "step 2500, validation accuracy 0.976\n",
      "step 2750, validation accuracy 0.984\n",
      "step 3000, validation accuracy 0.984\n",
      "step 3250, validation accuracy 0.984\n",
      "step 3500, validation accuracy 0.978\n",
      "step 3750, validation accuracy 0.982\n",
      "step 4000, validation accuracy 0.982\n",
      "step 4250, validation accuracy 0.98\n",
      "step 4500, validation accuracy 0.988\n",
      "step 4750, validation accuracy 0.992\n",
      "step 5000, validation accuracy 0.984\n",
      "step 5250, validation accuracy 0.984\n",
      "step 5500, validation accuracy 0.988\n",
      "step 5750, validation accuracy 0.992\n",
      "step 6000, validation accuracy 0.984\n",
      "step 6250, validation accuracy 0.99\n",
      "step 6500, validation accuracy 0.99\n",
      "step 6750, validation accuracy 0.982\n",
      "step 7000, validation accuracy 0.988\n",
      "step 7250, validation accuracy 0.988\n",
      "step 7500, validation accuracy 0.99\n",
      "step 7750, validation accuracy 0.99\n",
      "step 8000, validation accuracy 0.996\n",
      "step 8250, validation accuracy 0.988\n",
      "step 8500, validation accuracy 0.984\n",
      "step 8750, validation accuracy 0.988\n",
      "step 9000, validation accuracy 0.992\n",
      "step 9250, validation accuracy 0.988\n",
      "step 9500, validation accuracy 0.982\n",
      "step 9750, validation accuracy 0.988\n",
      "test accuracy 0.9905\n"
     ]
    }
   ],
   "source": [
    "# add bias\n",
    "x = tf.placeholder(tf.float32, [None, 784])\n",
    "y_ =tf.placeholder(tf.float32, [None, 10])\n",
    "\n",
    "xt = tf.reshape(x, shape=[-1, 28, 28, 1])\n",
    "W1 = tf.Variable(tf.truncated_normal([5,5,1,32], dtype=tf.float32, stddev=1e-1),trainable=True, name='w1')\n",
    "conv1 = conv2d(xt, W1)\n",
    "B1 = tf.Variable(tf.constant(0.0, shape=[28,28,32], dtype=tf.float32), trainable=True, name='b1')\n",
    "relu1 = tf.nn.relu(conv1) + B1\n",
    "pool1 = max_pool_2x2(relu1)\n",
    "\n",
    "W2 = tf.Variable(tf.truncated_normal([5,5,32,64], dtype=tf.float32, stddev=1e-1),trainable=True, name='w2')\n",
    "conv2 = conv2d(pool1, W2)\n",
    "B2 = tf.Variable(tf.constant(0.0, shape=[14,14,64], dtype=tf.float32), trainable=True, name='b2')\n",
    "relu2 = tf.nn.relu(conv2) + B2\n",
    "pool2 = max_pool_2x2(relu2)\n",
    "\n",
    "crp = tf.reshape(pool2, shape=[-1, 7 * 7 * 64])\n",
    "# Define the graph\n",
    "# refer to [02] jupter notebook\n",
    "weights_0 = tf.Variable(tf.truncated_normal([7 * 7 * 64, 1024], dtype=tf.float32, stddev=1e-1), name='weights_0')\n",
    "biases_0 = tf.Variable(tf.constant(0.0, shape=[1024], dtype=tf.float32), trainable=True, name='biases_0')\n",
    "FC_0 = tf.matmul(crp, weights_0) + biases_0\n",
    "\n",
    "NL_0 = tf.nn.relu(FC_0) # nonlinearity (Sigmoid/ReLU)\n",
    "\n",
    "weights_1 = tf.Variable(tf.truncated_normal([1024,10], dtype=tf.float32, stddev=1e-1), name='weights_1')\n",
    "biases_1 = tf.Variable(tf.constant(0.0, shape=[10], dtype=tf.float32), trainable=True, name='biases_1')\n",
    "FC_1 = tf.matmul(NL_0, weights_1) + biases_1\n",
    "\n",
    "y_conv = tf.nn.softmax(FC_1)# softmax\n",
    "### Create your CNN here##\n",
    "### Make sure to name your CNN output as y_conv ###\n",
    "\n",
    "\n",
    "\n",
    "# Loss \n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y_conv))\n",
    "\n",
    "# Optimizer\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "# Evaluation\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv, 1), tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    # Initialize all variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training regimen\n",
    "    for i in range(10000):\n",
    "        # Validate every 250th batch\n",
    "        if i % 250 == 0:\n",
    "            validation_accuracy = 0\n",
    "            for v in range(10):\n",
    "                batch = mnist.validation.next_batch(50)\n",
    "                validation_accuracy += (1/10) * accuracy.eval(feed_dict={x: batch[0], y_: batch[1]})\n",
    "            print('step %d, validation accuracy %g' % (i, validation_accuracy))\n",
    "        \n",
    "        # Train    \n",
    "        batch = mnist.train.next_batch(50)\n",
    "        train_step.run(feed_dict={x: batch[0], y_: batch[1]})\n",
    "\n",
    "    print('test accuracy %g' % accuracy.eval(feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
